Развлекательно-познавательный тест на знание теории вероятностей, python, машинного обучения и глубоких нейронных сетей.

https://docs.google.com/forms/d/e/1FAIpQLScjxrEKMxNUYG0WEE5jR7mLwbbTqhxQ8kv3UGV5Zng64g49Ng/viewform?fbzx=-6791607126993555012


* 1) Ниже изображен график функции распределения случайной величины  ξ.  Вычислите ее матожидание.
{-3: 0, -2: 0, -1: 0, 0: 1/3, 1: 1/3, 2: 1, 3: 1, 4: 1}
variants:
- 5/6
- 7/6
- 1.2
- 1
#+begin_src bash :results output
ls -al autoimgs
#+end_src

#+RESULTS:
: total 20
: drwxrwxr-x 1 u u    46 Mar  4 05:48 .
: drwxrwxr-x 1 u u    82 Mar  4 05:48 ..
: -rw-rw-r-- 1 u u 16523 Mar  4 05:48 calc-expected-value.png

#+begin_src python :results file graphics :exports both :file ./autoimgs/calc-expected-value.png :session s1
import matplotlib.pyplot as plt

x = list(range(-3,5))
y = [0, 0, 0, 1/3, 1/3, 1, 1, 1]
plt.plot(x,y)
plt.title("PMF")
plt.xlabel('outcome')
plt.ylabel('probability')
plt.savefig('./autoimgs/calc-expected-value.png')
# plt.close()
#+end_src

#+RESULTS:
[[file:./autoimgs/calc-expected-value.png]]


-3 -2 -1 = 0
0 = 1/3
1 = 0
2 = 2/3
3 = 0
4 = 0


#+begin_src python :results output :exports both :session s1
xy = {-3:0, -2:0, -1:0, 0:1/3, 1:0, 2:2/3, 3:0, 4:0}

print("E[X]=", sum([v[0]*v[1] for v in xy.items()]))
print("variants for answer=", [1, 7/6, 5/6, 1.2])
#+end_src

#+RESULTS:
: E[X]= 1.3333333333333333
: variants for answer= [1, 1.1666666666666667, 0.8333333333333334, 1.2]

Answer is 1.3

* 2) probability: three boxes
2. Есть три урны, в каждой из которых лежит по два шара (в первой —
 два черных, во второй — один черный и один белый, в третьей — два
 белых). Выбираем одну из этих урн случайным образом и вслепую
 вытаскиваем один из шаров. Он оказывается белым. Какова вероятность
 того, что второй шар в этой урне тоже белый?
- box1 - 2 balls - 2 black
- box2 - 2 balls - black and white
- box3 - 2 balls - 2 white

We can calculate this using Bayes' Theorem:

   P(A3|B) = P(B|A3) * P(A3) / P(B)

   We know the following probabilities:
   P(A1) = P(A2) = P(A3) = 1/3 (since the selection of urns is random)
   P(B|A1) = 0 (since urn 1 only contains black balls)
   P(B|A2) = 1/2 (since urn 2 contains one white ball out of two)
   P(B|A3) = 1 (since urn 3 contains two white balls out of two)

   Now, we need to calculate P(B), which is the probability of drawing a white ball regardless of
   the urn selected, by The Law of Total Probability:

   P(B) = P(B|A1) * P(A1) + P(B|A2) * P(A2) + P(B|A3) * P(A3)
   P(B) = 0 * 1/3 + 1/2 * 1/3 + 1 * 1/3
   P(B) = 1/6 + 1/6 + 1/3
   P(B) = 1/2

   Now, we can substitute these values into Bayes' Theorem:

   P(A3|B) = 1 * 1/3 / 1/2
   P(A3|B) = 2/3
* 3) select from fast to slow
: a[np.arange(0, len(a), 10)]
: np.take(a, np.arange(0, len(a), 10), axis=0)
: a[::10]
#+begin_src python :results output :exports both :session s1
import numpy as np
import random
a = []

for x in range(300000):
    a.append(random.randint(0,30999990))
    # a.append(str([random.randint(0,30) for x in range(random.randint(0,200))]))
a = np.array(a)

import time
start_time = time.time()
a[np.arange(0, len(a), 99000)]
print("-1-- %s seconds ---" % (time.time() - start_time))

start_time = time.time()
np.take(a, np.arange(0, len(a), 99000), axis=0)
print("-2-- %s seconds ---" % (time.time() - start_time))

start_time = time.time()
a[::99000]
print("-3-- %s seconds ---" % (time.time() - start_time))


#+end_src

#+RESULTS:
: -1-- 7.605552673339844e-05 seconds ---
: -2-- 6.103515625e-05 seconds ---
: -3-- 7.62939453125e-06 seconds ---

Answer is 3, 2, 1

a[::10] Slicing is a fundamental operation in Python and NumPy and
 is optimized for performance. Using slicing directly on the array is
 usually the fastest and most efficient way to select elements at
 regular intervals.

np.take(a, np.arange(0, len(a), 10), axis=0): This expression uses
 NumPy's np.take function to retrieve elements from a at specific
 indices provided by np.arange. The take function internally handles
 the indexing and can be more efficient than direct advanced
 indexing. However, it still involves additional function calls and
 array creation, which can impact performance.

a[np.arange(0, len(a), 10)]: This expression uses advanced indexing
 with an array generated by np.arange to select elements at specific
 indices. It involves the creation of an intermediate array and then
 indexing a based on that array. This approach can be slower compared
 to other methods because of the extra overhead involved in creating
 the index array.
* 4) select fastest shuffle method
1. permutation_1 = random.choice(list(permutations(elements)))
2. permutation_2 = tuple(np.random.permutation(elements))

permutation_1 don't use numpy and very memory hungry or have memory leakage.
Answer is 2. permutation_2

#+begin_src python :results output :exports both :session s1
from itertools import permutations
import random
import numpy as np
import time
elements = [str(x) for x in range(7)]
print(elements)
start_time = time.time()
permutation_2 = tuple(np.random.permutation(elements))
print("-1-- %s seconds ---" % (time.time() - start_time))

start_time = time.time()
permutation_1 = random.choice(list(permutations(elements)))
print("-2-- %s seconds ---" % (time.time() - start_time))
#+end_src

#+RESULTS:
: ['0', '1', '2', '3', '4', '5', '6']
: -1-- 0.0003056526184082031 seconds ---
: -2-- 0.0039136409759521484 seconds ---

* 5) which objects is allowed as arguments to functions?
5. Какие объекты можно передавать в качестве аргументов функции в Python?

all except modules

* 6) what deep of decision tree required?
Дана обучающая выборка x = [1, 2, 3, 4, 5], y = [1, 4, 9, 16,
 25]. Дерева решений какой глубины достаточно, чтобы ошибка регрессии
 на этой обучающей выборке была равна 0?

max depth is 3
#+begin_src python :results output :exports both :session s1
from sklearn import tree
x = [[1], [2], [3], [4], [5]]
y = [1, 4, 9, 16, 25]

clf = tree.DecisionTreeRegressor(max_depth=3)
clf = clf.fit(x, y)
from sklearn.metrics import accuracy_score
print(accuracy_score(y, clf.predict(x)))
print(clf.predict([[3]]))
print(clf.tree_.compute_node_depths())
#+end_src

#+RESULTS:
: 1.0
: [9.]
: [1 2 3 4 4 3 2 3 3]
* 7) look at image and say how it was done. ??
* 8) select which sentences is right/wrong
Какие утверждаения верны, а какие нет?
1. оценка качества моделей с помощью кросс-валидации Осмысленна только для шумных данных
2. оценка качества моделей с помощью кросс-валидации Обычно более точная
3. оценка качества моделей с помощью кросс-валидации Используется только для задач регрессии
4. оценка качества моделей с помощью кросс-валидации Требует кратно больше вычислений


1. Неверно. Оценка качества моделей с помощью кросс-валидации
осмысленна для любых данных.

2. Верно. Оценка качества с помощью кросс-валидации обычно точнее,
чем, например, оценка на основе разделения данных на
тренировочную и тестовую выборки.

3. Неверно. Кросс-валидация используется для задач классификации
и кластеризации так же, как и для задач регрессии.

4. Верно. Для оценки качества модели с помощью кросс-валидации
требуется выполнить несколько обучающих циклов, каждый из
которых состоит из нескольких этапов. Это занимает больше
времени, чем оценка, например, на основе тестовой выборки.
* 9) select which sentences is right/wrong
1. логистическая регрессия Используется для поиска нелинейной (сигмоидальной) разделяющей поверхности
2. логистическая регрессия Не может работать с бинарными признаками
3. логистическая регрессия На инференсе автоматически присваивает наблюдениям метку класса
4. логистическая регрессия Может использовать одновременно L1 и L2 регуляризацию

1. Верно. Логистическая регрессия используется для поиска
нелинейной (сигмоидальной) разделяющей поверхности.

2. Неверно. Логистическая регрессия может работать с бинарными
признаками.

3. Верно. На этапе инференса модель логистической регрессии
автоматически присваивает наблюдениям метку класса.

4. Верно. Модель логистической регрессии может использовать
одновременно L1 и L2 регуляризацию.
* 10) what optimization alogrithms for gradient descent (GD) have more momentum emphasize?
1. GD
2. GD+momentum с параметром  α
3. GD+momentum с параметром  β>α

answer: 3, 2, 1

* 11) Как производится inference при прохождении через слой inverted dropout'а с вероятностью исключения элемента  pdrop?

1. Несколько раз случайным образом зануляются элементы входного тензора, а затем результаты работы сети усредняются
2. Элементы входного тензора домножаются на 1-pdrop
3. Элементы входного тензора домножаются на 1/(1-pdrop)
4. Элементы входного тензора остаются без изменений

Answer is 3
* 12) In Deep learning Batch normilization can be expressed as formula X = v * (x - m)/\sqrt(s +e) + b.
Which of this v, m, s, b trained by gradient optimization and which set by other methods?

Слой batch normalization применяет следующее преобразование к входному тензору. Как обучаются параметры γ , β , μ  и σ ?


Answer:
v (gamma) and m (beta) are trained by gradient optimization methods
 along with the rest of the neural network parameters.

while s and b are updated using exponential moving averages.


s (variance): This variable represents the moving average of the variance of the input
   data. It is calculated during training and updated at each batch using exponential moving
   averages.

b (mean): This variable represents the moving average of the mean of the input data. Similar
   to the variance, it is calculated during training and updated at each batch using exponential
   moving averages.

* 13) conv(kernel_size = 7x7, strides = 1x1), how to redece trained parameters but keep 7x7 receptive field?
select variants

Дана операция conv(kernel_size = 7x7, strides = 1x1). Выберите способы снизить число обучаемых параметров этой операции, при этом сохраняя receptive field.

1. conv(kernel_size = 3x3, strides = 1x1, dilation=2x2)
2. conv(kernel_size = 3x3, strides = 2x2)
3. conv(kernel_size = 7x1, strides = 1x1) + conv(kernel_size = 1x7, strides = 1x1)
4. conv(ks = 3x3, strides = 1x1) + conv(ks = 3x3, strides = 1x1) + conv(ks = 3x3, strides = 1x1)

Answer: All of them.

Convolutional layer with kernel_size = 7x7: Number of parameters = (7 * 7 * input_channels + 1)
     * output_channels

   1. **conv(kernel_size = 3x3, strides = 1x1, dilation=2x2):**
      - With dilation of 2x2, you can reduce the number of parameters (as compared to a standard 7x7
        convolution) while still maintaining the effective receptive field of 7x7. This is a valid
        option for reducing parameters.

   2. **conv(kernel_size = 3x3, strides = 2x2):**
      - This variant will reduce the number of parameters significantly by reducing the spatial
        resolution after each convolution. It will maintain the same receptive field but at a
        reduced scale due to the larger stride. However, this may result in loss of fine-grained
        spatial information.

   3. **conv(kernel_size = 7x1, strides = 1x1) + conv(kernel_size = 1x7, strides = 1x1):**
      - This option aims to achieve a 7x7 receptive field by applying two separate convolutional
        layers with different kernel sizes. This can help in reducing the number of parameters while
        maintaining the desired receptive field, but it may not be as efficient in terms of
        computational cost compared to other options.

   4. **conv(ks = 3x3, strides = 1x1) + conv(ks = 3x3, strides = 1x1) + conv(ks = 3x3, strides =
   1x1):**
      - This approach utilizes multiple layers of standard 3x3 convolutions to cover the 7x7
        receptive field progressively. While this may work and maintain the receptive field, it may
        not be the most parameter-efficient option compared to the previous variants.
      - Total number of parameters for three separate convolutional layers = 3 * [(3 * 3 *
     input_channels + 1) * output_channels]

* 14) conv(kernel_size = 3x3, in_channels = 8, out_channels = 16, strides = 2x2, bias = True, padding = 'valid')
input is 32*32*8 (Height*Width*Channels), what count of parameters this layout have?

1. 3 * 3 * 8 * 16 + 16 = 1168
2. 3 * 3 * 32 * 32 * 8 * 16 + 16 = 1179664
3. 3 * 3 * (32 / 2) * (32 / 2) * 8 * 16 = 294912
4. 3 * 3 * 8 * 16 = 1152

Answer is 1.

Kernel Size: The kernel size is 3x3, which means each filter in the convolutional layer will
   have a size of 3x3.

   1. Input Channels (in_channels): The input has 8 channels.

   2. Output Channels (out_channels): The convolutional layer will output 16 channels.

   3. Strides: The strides are 2x2, which means the filter will move 2 pixels at a time during
   convolution.

   1. Bias: Bias is enabled (bias = True).

   2. Padding: Padding is 'valid', which means no zero-padding is added to the input.

   Now, let's calculate the number of parameters:

   For each output channel, we need a separate set of weights for each input channel:
k
   Number of weights per filter = Kernel Size x Input Channels = 3 x 3 x 8 = 72 weights

   In addition to the weights, there is also one bias parameter per output channel:

   Number of bias parameters = Output Channels = 16 biases

   Therefore, the total number of parameters in this layout is:

   Total Parameters = (Number of weights per filter + Number of biases) x Number of filters
   Total Parameters = (72 + 1) x 16
   Total Parameters = 73 x 16
   Total Parameters = 1168 parameters

   So, the convolutional layer with the given specifications has a total of 1168 parameters.
